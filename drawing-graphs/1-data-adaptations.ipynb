{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs, and the power of visualising datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this pair of notebooks, we are going to show how drawing meaningful graphs of large datasets can highlight interesting features that might have otherwise been missed when looking line-by-line at the data. These two notebooks are intended as a starting point, showing examples of two fairly common graphs, with the goal that you can then apply the same techniques to your own data sets. The first of these two notebooks is going to gather a dataset from Discovery and manipulate it into pandas dataframe. The second notebook will then use the matplotlib library to draw the graphs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we need to start by importing and installing required libraries. Note that this time, we are also gathering some pre-defined data - these are a couple of lists we are going to use to automate the calls to discovery to gather the data. The list of ship names is based on a list of ships of the Royal Navy from the War of 1812. The list of record series are some pre-selected series to limit our requests to Discovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q pandas\n",
    "%pip install -q json\n",
    "%pip install -q requests\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "\n",
    "import aditional_data\n",
    "\n",
    "record_series = aditional_data.admiralty_record_series\n",
    "ship_list = aditional_data.ships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What graphs are we going to draw?\n",
    "\n",
    "Discovery has a lot of metadata about each record available to use, so we are going to need to narrow things down a bit. For the purpose of these notebooks, we are going to take the position of a researcher into record descriptions - record description length can vary significantly (from a word to [several paragraphs](https://discovery.nationalarchives.gov.uk/browse/r/r/C10488311), within related records). As the description, along with the title, is one of the main ways a typical user would find a record, we are going to gather data and draw graphs to help illuminate the area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data \n",
    "\n",
    "To get the data we need we are going to use the Discovery API, similar to the [main series of notebooks](../1-intro-to-discovery-api.ipynb). As we have previously gone into it in detail, we are going to do this in one cell. This cell is going to run through the lists we imported earlier, use their data to build a request to the `search` endpoint, and select only the data we need from the results. As we have previously decided that we are interested in record descriptions and covering dates, we are only going to keep fields relating to those. \n",
    "\n",
    "As this request is going take some time, it is worth considering how we want to structure the json we want to build from the response, to help ensure we get the right data first time. Given that we are going to be looping through a list of names of ships, and extracting data from each record in response, we are going to initially build a dictionary like this, which is structured in a way where we can see the result of each query:  \n",
    "\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"ship\": \"[ship name]\",\n",
    "        \"data\": [\n",
    "            {\n",
    "                \"id\": \"[record id]\",\n",
    "                \"title\": \"[record title]\",\n",
    "                \"startDate\": \"[record start date]\",\n",
    "                \"endDate\": \"[record end date]\",\n",
    "                \"description\": \"[record description]\",\n",
    "                \"reference\": \"[record reference]\"\n",
    "            },\n",
    "            ...next record\n",
    "        ]\n",
    "    },\n",
    "    ...next ship\n",
    "]\n",
    "```\n",
    "\n",
    " Note: with the length of the list of ship names, this cell makes a lot of requests to Discovery and can take a minute or two to run. Also note that as this is an example only, we're only retrieving the default number of records with each query; in a real world scenario, you would want to spend time refining the query to ensure you retrieve all intended records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_data = []\n",
    "\n",
    "base_discovery_url = \"https://discovery.nationalarchives.gov.uk/API/search/records?\"\n",
    "\n",
    "for ship in ship_list:\n",
    "    url = base_discovery_url \n",
    "    for series in record_series:\n",
    "        url += series\n",
    "        url += \"&\"\n",
    "    url += \"sps.searchQuery=\" + ship\n",
    "    # print(url)                        # uncomment this line to see the url being used\n",
    "    headers = {'Accept': 'application/json'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response_json = response.json()\n",
    "    #print(response_json)               # if you want to see the full response to each query, uncomment this line\n",
    "    if response_json[\"records\"] != []:\n",
    "        found_data = []\n",
    "        for record in response_json[\"records\"]:\n",
    "            found_data.append(\n",
    "                {\n",
    "                    \"id\": record[\"id\"],\n",
    "                    \"title\": record[\"title\"],\n",
    "                    \"startDate\": record[\"startDate\"],\n",
    "                    \"endDate\": record[\"endDate\"],\n",
    "                    \"description\": record[\"description\"],\n",
    "                    \"reference\": record[\"reference\"].split(\"/\")[0] # here, we would normally get a reference such as ADM 1/1234, but we only want the ADM 1 part\n",
    "                }\n",
    "            )\n",
    "        ship_data.append(\n",
    "            {\n",
    "                \"ship\": ship,\n",
    "                \"data\": found_data\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        \n",
    "print(json.dumps(ship_data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a useful dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataframe is the equivelent of a single sheet in a spreadsheet, and is the format of data we are going to use when drawing graphs. There are [three rules](https://byuidatascience.github.io/python4ds/tidy-data.html) for a useful and tidy dataframe:\n",
    "\n",
    "1. Each variable forms a column\n",
    "2. Each observation forms a row\n",
    "3. Each value is a cell\n",
    "\n",
    "For the data we have retrieved from Discovery, records act as equivalent to observations, and the variables are the metadata. The data from Discovery is not arranged exactly like this yet; but it is close, and the data structure we created when we ran the requests is going to make it easy to get it into the right structure. We can then perform some checks and modifications to make it easier to work with, such as ensuring that columns are all stored as the correct data type, and checking for missing data.\n",
    "\n",
    "Note: we are showing one path to get the data into a dataframe, working with data from one endpoint from Discovery. This is a guide, not instructions, so when working with your own data, make sure you think through the steps you are going to take before you start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a large JSON file with all the data we want, as a result of the API calls in the previous cell. We also have a good understanding of what a nicely formatted dataframe looks like. The next stage is to use the `pandas` library to convert from the JSON to a dataframe. Doing so allows more powerful interactions with the dataframe, such as specifying the format of a column (e.g. that all values are dates), or filtering to only include rows that match a certain criteria.\n",
    "\n",
    "The first step we are going to take is to flatten the json. Having a flat json file makes it very straightforwards to convert to a dataframe; each record relates to an item in the json and a row in the dataframe, with each key in the json relating to a column in the dataframe. As with before, it can be valuable to draw the structure of the json to help visualise the result and ensure the conversion runs correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"ship\": \"[ship name]\",\n",
    "        \"id\": \"[record id]\",\n",
    "        \"title\": \"[record title]\",\n",
    "        \"startDate\": \"[record start date]\",\n",
    "        \"endDate\": \"[record end date]\",\n",
    "        \"description\": \"[record description]\",\n",
    "        \"reference\": \"[record reference]\"\n",
    "    },\n",
    "    ...next record\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_data_flat = []\n",
    "\n",
    "for ship in ship_data:\n",
    "    for record in ship[\"data\"]:\n",
    "        ship_data_flat.append(\n",
    "            {\n",
    "                \"ship\": ship[\"ship\"],\n",
    "                \"id\": record[\"id\"],\n",
    "                \"title\": record[\"title\"],\n",
    "                \"startDate\": record[\"startDate\"],\n",
    "                \"endDate\": record[\"endDate\"],\n",
    "                \"description\": record[\"description\"],\n",
    "                \"reference\": record[\"reference\"]\n",
    "            }\n",
    "        )\n",
    "\n",
    "print(json.dumps(ship_data_flat, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our flattened json, pandas makes it easy to convert it to a dataframe, as this next cell shows. We can then print the dataframe to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_data_frame = pd.DataFrame(ship_data_flat)\n",
    "\n",
    "print(ship_data_frame)\n",
    "\n",
    "## write to csv, even if the csv doesn't exist yet\n",
    "\n",
    "ship_data_frame.to_csv(\"ship_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-dataframe checks and modifications\n",
    "\n",
    "With the dataframe created, we can now inspect the date columns to ensure they are stored as the correct data type. Doing so will make it easier for us to do things like filtering by date, or calculating the difference between two dates. \n",
    "\n",
    "With pandas we can refer to a column in a dataframe using `dataframe['column name']`, similar to interacting with a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_data_frame[\"startDate\"] = pd.to_datetime(ship_data_frame[\"startDate\"], dayfirst=True)\n",
    "ship_data_frame[\"endDate\"] = pd.to_datetime(ship_data_frame[\"endDate\"], dayfirst=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a new column is also easy, requiring a very similar syntax. Here, we are going to add a column that calculates the difference between the start and end dates of each record - a step now possible as we have ensured that the dates are stored as the correct data type. We're also going to add a column that maps a colour to each unique ship name, and for each record series, which will make it easier to add colour to the graphs. A final column will number each ship - this one will be useful instead for selecting subsets of data. \n",
    "\n",
    "We'll finish off these additions by printing the dataframe again, to see what it looks like now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_data_frame[\"record_duration\"] = ship_data_frame[\"endDate\"] - ship_data_frame[\"startDate\"]\n",
    "\n",
    "ship_colour_map = {}\n",
    "for index, ship in enumerate(ship_list):\n",
    "    ship_colour_map[ship] = f\"C{index}\"\n",
    "\n",
    "ship_data_frame[\"ship_colour\"] = ship_data_frame[\"ship\"].map(ship_colour_map)\n",
    "\n",
    "ship_number_map = {}\n",
    "for index, ship in enumerate(ship_list):\n",
    "    ship_number_map[ship] = index\n",
    "\n",
    "ship_data_frame[\"ship_number\"] = ship_data_frame[\"ship\"].map(ship_number_map)\n",
    "\n",
    "record_series_colour_map = {}\n",
    "for index, series in enumerate(ship_data_frame[\"reference\"].unique()):\n",
    "    record_series_colour_map[series] = f\"C{index}\"\n",
    "\n",
    "ship_data_frame[\"record_series_colour\"] = ship_data_frame[\"reference\"].map(record_series_colour_map)\n",
    "\n",
    "ship_data_frame[\"description_length\"] = ship_data_frame[\"description\"].str.len()\n",
    "\n",
    "print(ship_data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for missing data is also easy, and can help us to identify any issues with the data. Here, we are going to check for missing data in the description column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing data in the description column\n",
    "\n",
    "missing_cell_count = ship_data_frame[\"description\"].isna().sum()\n",
    "\n",
    "print(\"There are \" + str(missing_cell_count) + \" missing cells in the description column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final thing to do before we move onto the next notebook is to save the dataframe as a csv file. This is a very useful feature of pandas, as it makes the data highly portable - it can be opened in excel or other spreadsheet software, or a different python script (re-opening it with pandas). Here, we are going to use it to let us open the data in the next notebook. Note that the CSV format does not store the data type of each column, so we will need to ensure that we re-apply the data type changes we made in this notebook when we re-open the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the dataframe to a csv file\n",
    "\n",
    "ship_data_frame.to_csv(\"ship_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
