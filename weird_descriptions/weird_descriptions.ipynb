{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weird descriptions\n",
    "\n",
    "Ever come across a newspaper headline that makes you think someone was just sticking random words together, but it turns out it was a real (if somewhat unusual) event? This notebook takes inspiration from those headlines, and runs through the steps to find weird descriptions in Discovery series.\n",
    "\n",
    "As with always, start by importing the necessary libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q json\n",
    "%pip install -q wordfreq\n",
    "%pip install -q matplotlib\n",
    "%pip install -q numpy\n",
    "import json\n",
    "from wordfreq import word_frequency\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import helper_functions as hf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is \"weird\" anyway?\n",
    "\n",
    "To do this reliably, start by defining metric of weirdness. The metric here is based on the idea that weird headlines tend to stick out because they have words that don't appear often, and in unusual combinations. As the main series of notebooks showed, its fairly straightforwards to get all the descriptions from a series, so lets assume access to that data. \n",
    "\n",
    "`record weirdness = average(average word weirdness from local, average word weirdness from global, average weirdness of word combinations)`\n",
    "\n",
    "Where: \n",
    " \n",
    "`word weirdness from local = 1 - (word frequency in all records in the series / total number of words in all records in the series)` - As a hypothetical example, if all records start \"The\", then have 100 different words that don't appear anywhere else, \"the\" should have a low score - its uncommon within individual records, but very common overall.\n",
    "\n",
    "`word weirdness from global` - There are datasets that provide word frequency in the English language which can be used to get a sense of how common a word is in general. \n",
    "\n",
    "`weirdness of word combinations = 1 - (pair frequency in all records in the series / toal number of pairs in the series)` - Looking at every adjacent pair of words in every record, how often are those two words next to each other? Similar to the word weirdness, if all descriptions start with \"The Parliament\", then have 100 different words that don't appear in other records, \"The Parliament\" should have a low score.\n",
    "\n",
    "\n",
    "These values are then averaged to accomodate for the length variability of record descriptions - some have lots of words, some have very few. If the weirdness value for each word was summed, then longer records would have a higher weirdness score than shorter records.\n",
    "\n",
    "## Lets start with getting the data\n",
    "\n",
    "As it was explained in detail in the [main series,](../1-intro-to-discovery-api.ipynb) this step of the process is in the `helper_functions.py` file to avoid repitition and making this notebook full of familiar code. Simply run the cell below to get the data - the default series it'll use is [`TITH`](https://discovery.nationalarchives.gov.uk/details/r/C254) but you can change that if you want. Note that really large series (eg `ADM`) will take a while to run, and may result in a timeout error from too many requests to the API. \n",
    "\n",
    "To make it easy to work through and work on the weirdness score, the data gathered from this first step is going to look like this (note the nested nature as we are gathering every record from a series - sub-series will be stepped into using the child endpoint of the API):\n",
    "\n",
    "```JSON\n",
    "{\n",
    "    \"series\": \"TITH\",\n",
    "    \"children\": [\n",
    "        {\n",
    "            \"series\": \"TITH 1\",\n",
    "            \"id\": \"discovery web id\",\n",
    "            \"description\": \"description of the record\",\n",
    "            \"children\" : [\n",
    "                {\n",
    "                    \"series\": \"TITH 1/1\",\n",
    "                    \"id\": \"discovery web id\",\n",
    "                    \"description\": \"description of the record\"\n",
    "                    \"children\": [ # and so on]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = hf.get_series_description(\"ACE\")\n",
    "\n",
    "print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data gathered, lets start working on the score. The first step is to see what words are in the descriptions, their frequnecy, and their weirdness score. The new data will be stored a new key to the data structure `word_weirdness` at the same level as the `children` key. The data in it will look like this:\n",
    "\n",
    "```JSON\n",
    "{\n",
    "    \"series\": \"TITH\",\n",
    "    \"children\": [as before],\n",
    "    \"total_words\": 1000,\n",
    "    \"word_weirdness\": [\n",
    "        {\n",
    "            \"word\": \"the\",\n",
    "            \"occurences\": 100,\n",
    "            \"weirdness\": 0.9,\n",
    "            \"word_frequency\": 0.1\n",
    "        },\n",
    "        {for every word in the description}\n",
    "    ]\n",
    "}\n",
    "```\n",
    "Word frequency, in this case, is being supplied by the [`wordfreq`](https://github.com/rspeer/wordfreq) package, which has a list of word frequencies in the English language, based on similar maths using a much larger dataset. You can read more about it in the link.\n",
    "\n",
    "These scores are based on all the descriptions in the data, not the descriptions of individual records, hence looping through every record and keeping a total score, rather than resetting the score each time. After this, the process for word pairs will be similar. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "word_weirdness = []\n",
    "\n",
    "def get_words(description):\n",
    "    if description[\"children\"] != \"No children\":\n",
    "        for child in description[\"children\"]:\n",
    "            get_words(child) # Recursion - if the record has children, call this function again with the child as the argument\n",
    "    if \"description\" in description: # The top level of the data doesn't have a description\n",
    "        if description[\"description\"] != None: # Sometimes the description is empty\n",
    "            for word in description[\"description\"].split(\" \"):\n",
    "                words.append(word)\n",
    "    return\n",
    "\n",
    "get_words(data)\n",
    "\n",
    "for unique_word in set(words):\n",
    "    word_weirdness.append(\n",
    "        {\n",
    "            \"word\": unique_word,\n",
    "            \"count\": words.count(unique_word),\n",
    "            \"weirdness\" : 1 - (words.count(unique_word) / len(words)),\n",
    "            \"word_frequency\": word_frequency(unique_word, \"en\", wordlist=\"large\")\n",
    "        }\n",
    "    )\n",
    "\n",
    "word_weirdness.sort(key=lambda x: x[\"weirdness\"], reverse=True) # This sorts the list in place, with the highest weirdness first\n",
    "\n",
    "data[\"total_words\"] = {\n",
    "    \"total\": len(words),\n",
    "    \"unique\": len(set(words))\n",
    "}\n",
    "data[\"word_weirdness\"] = word_weirdness\n",
    "\n",
    "print(json.dumps(data[\"word_weirdness\"], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you scroll through the results of that cell, you'll notice that the data have an very [long tail](https://en.wikipedia.org/wiki/Long_tail), plotting that later will allow a better look at the data. Its also interesting to note that, as word frequency scores are based on the English language, many of these words results in really tiny scores.\n",
    "\n",
    "## Word pairs\n",
    "\n",
    "This is a similar process to the above, with the main switch being that instead of isolating every word, we are isolating every pair. The output data structure is going to look essentially the same. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "pair_weirdness = []\n",
    "\n",
    "def get_pairs(description):\n",
    "    if description[\"children\"] != \"No children\":\n",
    "        for child in description[\"children\"]:\n",
    "            get_pairs(child) # Recursion - if the record has children, call this function again with the child as the argument\n",
    "    if \"description\" in description: # The top level of the data doesn't have a description\n",
    "        if description[\"description\"] != None: # Sometimes the description is empty\n",
    "            for i in range(len(description[\"description\"].split(\" \")) - 1):\n",
    "                pairs.append((description[\"description\"].split(\" \")[i], description[\"description\"].split(\" \")[i+1]))\n",
    "    return\n",
    "\n",
    "get_pairs(data)\n",
    "\n",
    "for unique_pair in set(pairs):\n",
    "    pair_weirdness.append(\n",
    "        {\n",
    "            \"pair\": unique_pair,\n",
    "            \"count\": pairs.count(unique_pair),\n",
    "            \"weirdness\" : 1 - (pairs.count(unique_pair) / len(pairs))\n",
    "        }\n",
    "    )\n",
    "\n",
    "pair_weirdness.sort(key=lambda x: x[\"weirdness\"], reverse=True) # This sorts the list in place, with the highest weirdness first\n",
    "\n",
    "data[\"pair_weirdness\"] = pair_weirdness\n",
    "\n",
    "print(json.dumps(data[\"pair_weirdness\"], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The big thing to notice here is that almost every pair is fairly unique, getting a score close to 1. This is to be expected, with the number of possible pairs being so large. For example, for 100 unique words, there would be 100! (100 factorial) possible pairs. This is a very large number, and a description will only have a tiny fraction of them.\n",
    "\n",
    "## Adding data to the original structure\n",
    "\n",
    "Now scores for each word have been calculated, the weirdness score for each record (using the definitions earlier in the notebook) can be added to the data structure. They will be added in as new key/values for each record. Given that the different scores for weirdness are so different, the average of the three, and the three scores themselves are included. Remember that the scores are being averaged to accomodate for varying description lengths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_description_scores(record):\n",
    "    if record[\"children\"] != \"No children\":\n",
    "        for child in record[\"children\"]:\n",
    "            add_description_scores(child)\n",
    "    if \"description\" in record:\n",
    "        if record[\"description\"] != None:\n",
    "            record[\"word_weirdness_score\"] = (sum([word[\"weirdness\"] for word in word_weirdness if word[\"word\"] in record[\"description\"].split(\" \")])/len(record[\"description\"].split(\" \")))\n",
    "            record[\"word_frequency_weirdness_score\"] = (sum([word[\"word_frequency\"] for word in word_weirdness if word[\"word\"] in record[\"description\"].split(\" \")])/len(record[\"description\"].split(\" \")))\n",
    "            record[\"pair_weirdness_score\"] = (sum([pair[\"weirdness\"] for pair in pair_weirdness if pair[\"pair\"] in [(record[\"description\"].split(\" \")[i], record[\"description\"].split(\" \")[i+1]) for i in range(len(record[\"description\"].split(\" \")) - 1)]])/len(record[\"description\"].split(\" \")))/2\n",
    "            record[\"average_weirdness_score\"] = (record[\"word_weirdness_score\"] + record[\"word_frequency_weirdness_score\"] + record[\"pair_weirdness_score\"])/3\n",
    "\n",
    "add_description_scores(data)\n",
    "\n",
    "print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data\n",
    "\n",
    "To start exploring the data, the first thing is to show the most and least weird records are. This is done by a similar recursive function, looking for the highest and lowest scores for average weirdness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look through the data and find the record with the highest average_weirdness_score\n",
    "\n",
    "hightest_weirdness = 0\n",
    "weirdest_record = {}\n",
    "\n",
    "def find_highest_weirdness(record):\n",
    "    global hightest_weirdness\n",
    "    global weirdest_record\n",
    "    if record[\"children\"] != \"No children\":\n",
    "        for child in record[\"children\"]:\n",
    "            find_highest_weirdness(child)\n",
    "    if \"average_weirdness_score\" in record:\n",
    "        if record[\"average_weirdness_score\"] > hightest_weirdness:\n",
    "            hightest_weirdness = record[\"average_weirdness_score\"]\n",
    "            weirdest_record = record\n",
    "\n",
    "find_highest_weirdness(data)\n",
    "\n",
    "print(json.dumps(weirdest_record, indent=4))\n",
    "\n",
    "lowest_weirdness = 1\n",
    "least_weird_record = {}\n",
    "\n",
    "def find_lowest_weirdness(record):\n",
    "    global lowest_weirdness\n",
    "    global least_weird_record\n",
    "    if record[\"children\"] != \"No children\":\n",
    "        for child in record[\"children\"]:\n",
    "            find_lowest_weirdness(child)\n",
    "    if \"average_weirdness_score\" in record:\n",
    "        if record[\"average_weirdness_score\"] < lowest_weirdness:\n",
    "            lowest_weirdness = record[\"average_weirdness_score\"]\n",
    "            least_weird_record = record\n",
    "\n",
    "lowest_weirdness = 1\n",
    "\n",
    "find_lowest_weirdness(data)\n",
    "\n",
    "print(json.dumps(least_weird_record, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the long tail in the word count scores? Lets plot that to see how long it is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_and_count = {}\n",
    "\n",
    "for word in data[\"word_weirdness\"]:\n",
    "    word_and_count[word[\"word\"]] = word[\"count\"]\n",
    "\n",
    "plt_word_counts, ax_word_counts = plt.subplots(figsize=(30, 30))\n",
    "ax_word_counts.bar(word_and_count.keys(), word_and_count.values())\n",
    "ax_word_counts.set_xlabel('Word')\n",
    "ax_word_counts.set_ylabel('Count')\n",
    "ax_word_counts.set_title('Word count')\n",
    "ax_word_counts.set_xticks([]) # This stops the x-ticks from being displayed - there are too many words to display them all\n",
    "ax_word_counts.yaxis.set_minor_locator(plt.MultipleLocator(1))\n",
    "ax_word_counts.yaxis.set_minor_formatter(plt.FuncFormatter(lambda x, _: int(x)))\n",
    "ax_word_counts.grid(which=\"minor\", axis=\"y\", linestyle=\"--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, that's a long tail! A lot of words only appear once, with most only appearing fewer than 10 times. \n",
    "\n",
    "To see how the weirdness score is distributed, lets plot that too. Here we're going to use a histogram, but with less refinement than other graphs, the goal is an overview of distribution of the scores. Remember that the record data is nested, so we'll need to step into the sub-series to get the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_and_weirdness = {}\n",
    "\n",
    "def get_id_and_weirdness(record):\n",
    "    if record[\"children\"] != \"No children\":\n",
    "        for child in record[\"children\"]:\n",
    "            get_id_and_weirdness(child)\n",
    "    if \"id\" in record:\n",
    "        if \"average_weirdness_score\" in record:\n",
    "            id_and_weirdness[record[\"id\"]] = record[\"average_weirdness_score\"]\n",
    "\n",
    "get_id_and_weirdness(data)\n",
    "\n",
    "plt_id_and_weirdness, ax_id_and_weirdness = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "ax_id_and_weirdness.bar(id_and_weirdness.keys(), id_and_weirdness.values())\n",
    "ax_id_and_weirdness.set_xlabel('ID')\n",
    "ax_id_and_weirdness.set_xticks([])\n",
    "ax_id_and_weirdness.set_ylabel('Weirdness')\n",
    "ax_id_and_weirdness.set_title('Weirdness by ID')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the default series, this shows that most records are roughly the same weirdness. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some starter stats\n",
    "\n",
    "The weirdest record has been found, but a quick statistical analysis will give more confidence in saying \"its weird\". Lets find out the mean and standard deviation of the scores. If our record is more than 3 standard deviations from the mean, then it can be said to be statistically weird. As these are very widely used metrics, `numpy` makes this very approachable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_weirdness = np.mean(list(id_and_weirdness.values()))\n",
    "standard_deviation_weirdness = np.std(list(id_and_weirdness.values()))\n",
    "\n",
    "print(f\"The mean weirdness is {mean_weirdness} and the standard deviation is {standard_deviation_weirdness}\")\n",
    "\n",
    "# Reprint the weirdest record\n",
    "\n",
    "print(json.dumps(weirdest_record, indent=4))\n",
    "\n",
    "# see if the weirdest record is out by more than 3 standard deviations, else print the average weirdness score it would need to be an outlier\n",
    "\n",
    "if weirdest_record[\"average_weirdness_score\"] > mean_weirdness + 3 * standard_deviation_weirdness or weirdest_record[\"average_weirdness_score\"] < mean_weirdness - 3 * standard_deviation_weirdness:\n",
    "    print(\"The weirdest record is an outlier\")\n",
    "else:\n",
    "    print(f\"The weirdest record is not an outlier, it would need an average weirdness score of {mean_weirdness + 3 * standard_deviation_weirdness} or { mean_weirdness - 3 * standard_deviation_weirdness} to be one.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the data\n",
    "\n",
    "After a lot of analysis work, it makes sense to save the data. We're going to save it as a JSON file, as that's the format we've been working with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data to a file called data.json\n",
    "\n",
    "with open(\"data.json\", \"w\") as file:\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we've gone through the full process of:\n",
    "1. From an inspiration, defining a metric to analyse our input data\n",
    "2. Gathering the correct parts of our data\n",
    "3. Calculating our metric for each record\n",
    "4. Visualising the results\n",
    "5. A little bit of statistical analysis\n",
    "6. Saving the data\n",
    "\n",
    "Or, what appeared to be a light hearted look at descriptions was sneakily a full data analysis project (if a small one). Hopefully this notebook has given you a good idea of how to approach a similar problem, and you had fun along the way."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
